%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               My System 24pp
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Prototype}
\label{sec:implementation}

\section*{Summary}
A SleeveAR prototype was built to meet our vision expectations. To implement all planned features, our prototype had to rely on some already existing devices, mainly for motion tracking and feedback sources. This chapter will being with a description of our architecture and its most notable implementation details






\section{Architecture}
\label{sec:impl:arch}

The SleeveAR architecture, which can be seen in figure \ref{fig:arch}, relies on several devices to both receive and send information.
In terms of input, we are receiving real-time tracking information through an UDP connection with a tracking dedicated computer. Section \ref{sec:impl:trackdevices} and \ref{prototype-tracking} will explain with more detail what devices were used and how did we actually were able to track a user's arm by using a sleeve with markers.

Given the real-time tracking, our SleeveAR prototype will then generate feedback accordingly to a specific exercise the user should attempt to execute. To provide feedback, we controlled speakers to provide audio notifications but most importantly, a light projector to project information both on the user's arm and floor. In Section \ref{prototype-projection} we explain how this projection was achieved.

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{imgs/impl/arch.jpg}
    \end{center}
    \caption{SleeveAR Architecture.}
    \label{fig:arch}
\end{figure}




\section{Tools}
\label{sec:impl:tools}

\subsection{Tracking Devices}
\label{sec:impl:trackdevices}

As for tracking devices, we had two different options to choose from.
The first makes use of the recently released, Microsoft Kinect One\footnote{\url{http://www.xbox.com/xboxone/kinect}} 
(previously baptized as Kinect 2), which supposedly offers a better tracking quality than 
the previous version, Kinect 1. Although this might be true, for our implementation we wanted a much more accurate and faster source of tracking, while also with a not so common tendency to fail due to camera occlusion.

The other alternative available at our laboratories was an OptiTrack Motion Capture system \footnote{\url{https://www.naturalpoint.com/optitrack/}}. 
This option offered us a more precise tracking and the possibility of dealing with occlusions due to the multiple cameras scattered around the room. 
The downside is the fact that it requires body markers to be used in order to successfully detect a person, unlike the 
Kinect which detects the human body through software algorithms. 
But using a comfortable and rather easy way to attach these body markers, made this not a bigger issue as it seems. 
A description of how we used the body markers can be found in section \ref{prototype-tracking}.



\subsection{Feedback Devices}

Providing feedback could be considered one of the foundations of this work. 
We chose to provide both visual and auditory feedback, being the latter much less vital to our goals in this implementation.

As it was described in section \ref{sec:sleevear}, our planned visual feedback would be applied on the user's arm and floor.
We relied on a light projector attached to the ceiling of our laboratory to project all visual feedback.
Details about how the light projections were able to hit the correct places, specifically the user's moving arm and floor, will be explained in section \ref{prototype-projection}.

Audio feedback was used simply to notify the user when to start the exercise and \todo{COMPLETAR}.
To provide audio, we relied on a speaker system also available at our laboratory

\subsection{Software}

We chose to implement our prototype with the well known \emph{Unity3D} game engine\footnote{\url{http://www.unity3d.com}}.
This engine already provides several tools that facilitate the development of augmented reality applications and we have in our possession already developed frameworks to communicate with the available tracking devices. In addition to this, Unity3D uses \texttt{C\#} as his main programming language, which is one of the most common languages use in the game development world, and already offers a wide range of solutions to create visual information.


\begin{figure}[!t]
\minipage[t]{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/impl/laboratory.jpg}
    \caption{Work Laboratory.}
    \label{fig:laboratory}
    \endminipage\hfill
\minipage[t]{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/impl/projector.png}
    \caption{Light Projector.}
    \label{fig:projector}
    \endminipage\hfill
\minipage[t]{0.32\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{imgs/impl/optitracksensor.png}
    \caption{Single Optitrack Camera.}
    \label{fig:optitracksensor}
    \endminipage
\end{figure}

\section{Setup Environment}

All the work here presented was conducted in the Jo\~ao Louren\c{c}o Fernandes Laboratory, located at Campus Taguspark of T\'ecnico Lisboa, seen at figure \ref{fig:laboratory}.
In this laboratory we had at our disposal all required devices to implement our work.

There were Optitrack motion sensors already fixed on the walls and prepared to use UDP communication to send tracking data. 
In section \ref{prototype-tracking} we will further explain the key points about our tracking system.

The light projector is a short-throw Benq MP780 ST+, attached to the ceiling, as seen in figure \ref{fig:projector}, and
was used by connecting a VGA cable to our working computer. We used a resolution of \textit{1280x1024} which resulted in a floor projection of approximately \textit{4.3x3.3} 
meters.

\section{Implementation}

\subsection{Tracking}
\label{prototype-tracking}

\begin{figure}
\minipage[t]{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/impl/singlemarker.png}
    \caption{Single Tracking Marker.}
    \label{fig:singlemarker}
    \endminipage\hfill
\minipage[t]{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/impl/markercombination.png}
    \caption{Marker Combination.}
    \label{fig:markercombination}
    \endminipage\hfill
\minipage[t]{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/impl/rigidbodiesattached.png}
    \caption{Markers location on arm.}
    \label{fig:rigidbodiesattached}
    \endminipage
\end{figure}


As previously stated, we chose the Optitrack as our tracking system to implement SleeveAR's approach. This tracking system relies on body markers to capture movement.
These body markers are made of reflective material and are usually shaped as small spheres as we can see in figure \ref{fig:singlemarker}.
But Optitrack is not able to track one single marker, instead, we need to use combinations of markers for it to calculate both 
position and rotation of the combination's center of mass. 
To create combinations, we used small plastic objects onto which was possible to attach several markers exactly as the one being shown in figure \ref{fig:markercombination}.

After combining at least three markers, they could be assigned an ID inside Optitrack software. 
From then on, the software was able to identify that specific combination and provide us with its current position and rotation.
For easier understanding, and writing, of this section, we will name this markers combination as a \textbf{rigid body}.

With this in mind, for our work, we required three different rigid bodies. Each one should be attached to a different arm location, in this case, shoulder, elbow and wrist. 
By doing so, we were able to receive tracking data from the three locations, and therefore, replicate the arm with data. In figure \ref{fig:rigidbodiesattached} we can observe
an approximate location of each rigid body.

Our first method of placing the each rigid body was by using a Velcro bracelet around each location of the arm. 
Each rigid body would then be attached to each bracelet. This method didn't have a positive result for several reasons. 
First, it took to long to attach each bracelet around the arm, in addition, the Velcro material provoked discomfort when pressed hard against the skin. 
Second, the bracelets tended to move out of place, especially in the shoulder area where it was particularly hard to properly hold it in place.

Having an easy way to attach and hard to move method of holding our rigid bodies was vital for our work. 
Rigid bodies moving out of place during a movement could result in unwanted and unexpected results. 
Therefore, we created a better attachment method, by using a custom designed sleeve.

%\subsubsection{Sleeve}

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=\textwidth]{imgs/impl/sleevewearable.png}
    \end{center}
    \caption{Sleeve used for tracking.}
    \label{fig:sleevewearable}
\end{figure}

We designed a custom sleeve, seen at figure \ref{fig:sleevewearable}, made out of wool material. 
This solved our previously presented problems by fixing the sleeve in place using a kind of "belt" around the user's torso which greatly increased its stability. 
Each of the rigid bodies were still attached to a bracelet, but in this case the bracelets were stitched to the sleeve. 
This improved significantly the rigid bodies attachment due to the bracelets never leaving the sleeve, 
while also enabling us to still squeeze them more or less depending on the user's arm thickness.
Another advantage of using our custom sleeve is providing a better surface to project information, due to it being white. This enable us to have a smoother and more neutral surface to project color for example.  

\subsection{Projection}
\label{prototype-projection}


Projection visual information on the floor and user's arm was one of the greatest challenges in our implementation. To accomplish it, we divided the implementation into smaller goals.

First of all, we needed to understand the requires concepts to project information wherever we wanted. We'll use a simple example with a cube being tracked by 
our device and to facilitate we'll use the nomenclature \todo{physical position, virtual position, projected position}.

\todo{fig 1 do caderno}

Looking at \todo{fig 1}, we have a cube inside our room being tracked by our optitrack sensor, hence, we receive its raw positional data, i.e., its \todo{virtual position}.
So, now we know the cube's position in a virtual room. If we wanted to project, for example, a small circle directly below the cube, we would encounter several obstacles.


\todo{fig 4 do caderno}

Even though we might have the circle being projected perfectly below \todo{fig 4, A}, it was not constant.
If we moved the cube to another position, closer to the edges, like in cubes B and C from \todo{fig 4}, the circle would not remain below it. 
Two reasons could cause this. First, the projected area center is not synced with the tracking coordinates center. 
This already generates a small offset in our projection. 
Second, and probably what influences our projection accuracy the most, is the projection distortion closer to the edges.
Looking at \todo{figura4}, we can see the circle projection getting further away from the desired position if we move the cube between the positions A, B and C.

The first solution to this problem was discovering the correct ratio between what is being tracked what is being projected on the floor. 
If we applied this ratio on our projected content, it could diminish the observed offset. 
Our second solution was to scale the virtual content in order to meet the physical dimensions of the room. 
In other words, looking at \todo{fig 5}, if we projected a line with a virtual length of one unity unit \todo{acronym}, the resulting projection on the floor should have a length of approximately one meter.
By doing this, the offsets observed were almost non-existing and even if they were noticeable, we could easily fixed them with some manual calibration.

\todo{fig 5 do caderno}

After being able to project information directly below any tracked object, our next challenge was to project light on the object being tracked. 
In this case, based on our example, we want to project the circle on top of the cube. 
Our challenge here was discovering how a two-dimensional projection could be used to illuminate targets above the floor.

\todo{fig 6}

\todo{fig 7}

Looking at \todo{fig 6}, we have a side-view of our example. When the projector light hits the cube, a shadow is created on the floor. 
Since this shadow is still inside the projected area, if we move the circle to its position, it will then be projected on top of the cube.

If we are able to project information where we want (when it comes to floor positions), to hit the cube with light, we just need to know where the shadow is in terms of virtual position. 
To accomplish this, we calculated the projector virtual position and then use it to predict where the shadow would be. 
By simulating the direction a light ray would have when originating from the projector and aimed at a cube, we could achieve a shadow virtual position. 
As is can be seen in fig \todo{fig 7}, by using the "lightdir" vector, we reach the shadow virtual position. 
From there we are already able to convert it to the correct physical position. Hence, we are able to hit the actual cube with the circle projection.
When applying this line of thought to our tracked rigid bodies, we were able to project any kind of content on top of the user's arm

\subsection{Recording Movements}

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=\textwidth]{imgs/impl/recordinginterface}
    \end{center}
    \caption{Sleeve used for tracking.}
    \label{fig:recordinginterface}
\end{figure}


As it was described in section \ref{sec:sleevear:approach}, our prototype should be able to record demonstrated exercises for further use.
We implemented a simple interface to facilitate the recording process.

Assuming the user is already wearing our tracking sleeve, the \textbf{record} button, seen at figure \ref{fig:recordinginterface}, simply had to be pressed to enable recording. 
After being pressed, an audio countdown was played through the laboratory speakers, in order to give the user some time to place himself in the desired exercise initial position. 
By standard, the recording time was set to a maximum duration of ten seconds, while capturing the arm tracking data 24 times per second.
The user could also stop the recording earlier if the exercise was not intended to take 10 seconds. He simply had to press the stop button.

Immediately after finishing recording, new options appear in the interface. 
The user could fill a text area with the intended file name and save the exercise, or first attach some more information to the file which we will explain next.

As we shown previously in section \ref{sec:movementguidance}, visual feedback provided during a movement included drawing the movement path on the floor. 
But this could become confusing if, for example, the recorded movement passed by the same place two times. 
The drawn path would intersect with itself, creating an unclear clue to where the movement might go next for the user. 
To avoid this, we implemented the \textbf{exercise parts}, which basically allowed us to divide the exercise in different parts and, consequently, divide also the drawn trajectory generated by our prototype.

To divide an exercise in parts, a slider was available in our interface which, when dragged, allowed to replay back and forth the recorded exercise. 
If the "Add Part" button was pressed, a division would be created in the the exercise based on where the slider currently was positioned.
All of this information was saved along with the exercises and stored on our computer.



\subsection{Data Storage}

Our stored files contained a list of all captured data from the three rigid bodies, as described in \ref{prototype-tracking}. 
Each of the list entries contained the position and rotation from each rigid body and also the required information to identify the different exercise parts in case they were assigned. 
If not, the exercises would be treated as one full movement without division.

We chose to save our information in the JSON format as oppose to XML due to generating much smaller file sizes. In addition to that, is was also a much more readable format, something we found useful when implementing and debugging this part of our prototype.

\subsection{Guiding}

\todo{figura feedback implementado}

Guiding a user through a recorded exercise, the core of our work, involved several phases for it to be possible.

First of all, we needed to load exercise files for them to be used again. 
We decided to use an already existing library, FullSerializer\footnote{\url{https://github.com/jacobdufault/fullserializer}}, for the sole purpose of reading and parsing JSON files.
After loading an exercise file, we could then start guiding a user whenever we wanted.

To implement the planned visual feedback, described in section \ref{vision-feedback}, and for it to dynamically change throughout a movement, 
we divided responsibilities into two main components. We will refer to these components as services, more specifically, the \ac{FS} and \ac{ES}.
The \ac{FS} has the responsibility of manipulating the provided feedback while the \ac{ES} was responsible in deciding if the user was executing correctly the exercise.

In other words, considering an exercise as a list of specific arm directions that we want another person to replicate in the correct orders. 
Obviously, the exercise should start in the first entry of this list. 
Once the user gets close enough to these specific entry, the \ac{ES} will then advance to the next entry. 
This will keep happening until we reach the end of the list and the exercise is over.

With this in mind, every time \ac{ES} is focused on a specific entry, the information inside it will generate the so called visual feedback \textbf{desired state}. 
Therefore, grabbing the concept of current and desired state from section \ref{vision-feedback}, two things will happen. 
First, \ac{FS} will update the current state based on real-time tracking from our current user. 
Second, the desired state will be updated accordingly to the current entry \ac{ES} is focusing on.

Even though we were tracking rigid bodies positions, we could not blindly compare positions between the original and attempt.
As explained in section \ref{sec:skeletoncomparison}, the person which recorded an exercise could have, for example, a different arm length.
With this in mind, even though we were storing rigid bodies positions, we generated normalized vectors to represent the arm direction. 
Upper arm direction was a normalized vector pointing from should to elbow position, while the fore arm direction was a vector from elbow to wrist.
By comparing both arm region directions, we eliminate the physical differences between different user's arm.

\todo{JV:should I have a more low-level explanation of this? I wanted it to be "simple"}
\todo{JV:talk about thresholds?}

\subsection{Performance Review}

\todo{figura performance review}

Every time an exercise was finished, our prototype should present the user with a review of his attempt. 
As described in our approach, the review should contain trajectory comparison between the original exercise and user's attempt.
As for the score, we implemented a simple formula to return a value from 0 to 100. 
Unfortunately, only after finishing our user tests we discovered a better alternative, which we ended up using for our result analysis, in section \ref{sec:results}.
A picture of our performance review implementation can be seen in figure \todo{figura performance review}
