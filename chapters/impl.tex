%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               My System 24pp
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Prototype}
\label{sec:implementation}

\section*{Summary}
A SleeveAR prototype was built to meet our vision expectations. To implement all planned features, our prototype had to rely on some already existing devices, mainly for motion tracking and feedback sources. This chapter will being with a description of our architecture and its most notable implementation details



\section{Architecture}
\label{sec:impl:arch}



\section{Tools}
\label{sec:impl:tools}

\subsection{Tracking Devices}


As for tracking devices, we had two different options to choose from.
The first makes use of the recently released, Microsoft Kinect One\footnote{\url{http://www.xbox.com/xboxone/kinect}} 
(previously baptized as Kinect 2), which supposedly offers a better tracking quality than 
the previous version, Kinect 1. Although this might be true, for our implementation we wanted a much more accurate and faster source of tracking, while also with a not so common tendency to fail due to camera occlusion.

The other alternative available at our laboratories was an OptiTrack Motion Capture system \footnote{\url{https://www.naturalpoint.com/optitrack/}}. 
This option offered us a more precise tracking and the possibility of dealing with occlusions due to the multiple cameras scattered around the room. 
The downside is the fact that it requires body markers to be used in order to successfully detect a person, unlike the 
Kinect which detects the human body through software algorithms. 
But using a comfortable and rather easy way to attach these body markers, made this not a bigger issue as it seems. 
A description of how we used the body markers can be found in section \ref{prototype-sleeve}.



\subsection{Feedback Devices}

Providing feedback could be considered one of the foundations of this work. 
We chose to provide both visual and auditory feedback, being the latter much less vital to our goals in this implementation.

As it was described in section \ref{sec:sleevear}, our planned visual feedback would be applied on the user's arm and floor.
We relied on a light projector attached to the ceiling of our laboratory to project all visual feedback.
Details about how the light projections were able to hit the correct places, specifically the user's moving arm and floor, will be explained in section \ref{prototype-projection}.

Audio feedback was used simply to notify the user when to start the exercise and \todo{COMPLETAR}.
To provide audio, we relied on a speaker system also available at our laboratory

\subsection{Software}

We chose to implement our prototype with the well known \emph{Unity3D} game engine\footnote{\url{http://www.unity3d.com}}.
This engine already provides several tools that facilitate the development of augmented reality applications and we have in our possession already developed frameworks to communicate with the available tracking devices. In addition to this, Unity3D uses c# as his main programming language, which is one of the most common languages use in the game development world, and already offers a wide range of solutions to create visual information.

\section{Setup Environment}

\section{Implementation}

\subsection{Sleeve}
\label{prototype-sleeve}

\subsection{Projection}
\label{prototype-projection}

\todo{mais cenas}







\todo{LIMPAR ABAIXO DISTO}



\section{Development}
\label{sec:impl:dev}
